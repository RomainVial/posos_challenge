{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = {'train': {'question':[], 'intention':[]},\n",
    "           'test': {'question':[]}}\n",
    "\n",
    "with open('data/input_train.csv') as csvfile:\n",
    "    index = 0\n",
    "    reader = csv.reader(csvfile, delimiter=';', )\n",
    "    reader.next()\n",
    "    for row in reader:\n",
    "        assert index == int(row[0]), '{} {}'.format(index, row[0])\n",
    "        dataset['train']['question'].append(row[1])\n",
    "        index += 1\n",
    "\n",
    "with open('data/output_train.csv') as csvfile:\n",
    "    index = 0\n",
    "    reader = csv.reader(csvfile, delimiter=';', )\n",
    "    reader.next()\n",
    "    for row in reader:\n",
    "        assert index == int(row[0]), '{} {}'.format(index, row[0])\n",
    "        dataset['train']['intention'].append(row[1])\n",
    "        index += 1\n",
    "\n",
    "with open('data/input_test.csv') as csvfile:\n",
    "    index = len(dataset['train']['question'])\n",
    "    reader = csv.reader(csvfile, delimiter=';', )\n",
    "    reader.next()\n",
    "    for row in reader:\n",
    "        assert index == int(row[0]), '{} {}'.format(index, row[0])\n",
    "        dataset['test']['question'].append(row[1])\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore class balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print dataset['intention'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load RCP and components dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rcp = {}\n",
    "components = {}\n",
    "with open('data/CIS.txt') as textfile:\n",
    "    lines = map(lambda l: l.strip('\\r\\n').split('\\t'), textfile.readlines())\n",
    "    for line in lines:\n",
    "        code_cis, name = int(line[0]), line[1]\n",
    "        rcp[code_cis] = {\n",
    "            'name': name,\n",
    "            'commercialization': line[-3] == 'Commercialis\\xe9e',\n",
    "            'composition': []\n",
    "        }\n",
    "\n",
    "with open('data/COMPO.txt') as textfile:\n",
    "    lines = map(lambda l: l.strip('\\r\\n').split('\\t'), textfile.readlines())\n",
    "    for line in lines:\n",
    "        code_cis, code_compo, name, type = int(line[0]), int(line[2]), line[3], line[1]\n",
    "        rcp[code_cis]['composition'].append(code_compo)\n",
    "        components[code_compo] = {\n",
    "            'name': name,\n",
    "            'type': type\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean names to only keep drug name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subre = '&|mg|ml|mL|POUR CENT|POUR MILLE|h|heures|g|\\%|UI|U.I.|SANS SUCRE|microgrammes|I.V.|dose|ENFANTS|IM|IR|ENFANTS ET NOURRISSONS|ADULTES|U.CEIP'\n",
    "for cis_code in rcp:\n",
    "    #print rcp[cis_code]['name']\n",
    "    rcp[cis_code]['clean_name'] = rcp[cis_code]['name'].split(' ')[0].strip(' ,')\n",
    "    #print rcp[cis_code]['clean_name']\n",
    "    '''\n",
    "    match = re.match(r\"(([A-Z.\\/ \\(\\)\\-])+)([0-9,. \\/]*(\"+ subre +\")[\\/]*)*, .*\", rcp[cis_code]['name'])\n",
    "    if match:\n",
    "        rcp[cis_code]['clean_name'] = match.group(1).split(' ')[0].strip(' ')\n",
    "        #print rcp[cis_code]['clean_name']\n",
    "    else:\n",
    "        #print rcp[cis_code]['name']\n",
    "        pass\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'composition': [83934], 'clean_name': 'ROPIVACA\\xcfNE', 'name': 'ROPIVACA\\xcfNE KABI 10 mg/ml, solution injectable en ampoule', 'commercialization': True}\n"
     ]
    }
   ],
   "source": [
    "print rcp[69133501]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract and clean sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ftfy\n",
    "import spacy\n",
    "import pickle as pkl\n",
    "from textacy import preprocess_text\n",
    "\n",
    "nlp = spacy.load('fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_corpus(split):\n",
    "    for idx in range(len(dataset[split]['question'])):\n",
    "        if idx+1 > 20:\n",
    "            pass\n",
    "        if (idx+1) % 100 == 0:\n",
    "            print '{} out of {}'.format(idx+1, len(dataset[split]['question']))\n",
    "\n",
    "        # Preprocess question\n",
    "        question = dataset[split]['question'][idx]\n",
    "        question = preprocess_text(question.decode('utf-8'), fix_unicode=True, no_accents=True, lowercase=True)\n",
    "        tokens = [token.lower_ for token in nlp(question)]\n",
    "        \n",
    "        # Extract intention\n",
    "        if dataset[split].has_key('intention'):\n",
    "            corpus[split]['labels'].append(dataset[split]['intention'][idx])\n",
    "            \n",
    "        # Update corpus with processed texts\n",
    "        corpus[split]['raw_texts'].append(question)\n",
    "        \n",
    "        med_idx = 0\n",
    "        for cis_code in rcp:\n",
    "            if rcp[cis_code].has_key('clean_name') and rcp[cis_code]['commercialization']:\n",
    "                if len(rcp[cis_code]['clean_name']) <= 4:\n",
    "                    continue\n",
    "\n",
    "                _, encoding = ftfy.guess_bytes(rcp[cis_code]['clean_name'])\n",
    "                cis_name = preprocess_text(rcp[cis_code]['clean_name'].decode(encoding),\n",
    "                                           fix_unicode=True, no_accents=True, lowercase=True)\n",
    "\n",
    "                if cis_name in tokens:\n",
    "                    question = question.replace(cis_name, 'MED{}'.format(med_idx))\n",
    "                    tokens.remove(cis_name)\n",
    "                    med_idx += 1\n",
    "        \n",
    "        #print dataset[split]['question'][idx]\n",
    "        #print question\n",
    "\n",
    "        corpus[split]['texts_with_numbered_med'].append(question)\n",
    "        corpus[split]['texts_with_med'].append(re.sub(r'MED[0-9]', u'MED', question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "100 out of 2035\n",
      "200 out of 2035\n",
      "300 out of 2035\n",
      "400 out of 2035\n",
      "500 out of 2035\n",
      "600 out of 2035\n",
      "700 out of 2035\n",
      "800 out of 2035\n",
      "900 out of 2035\n",
      "1000 out of 2035\n",
      "1100 out of 2035\n",
      "1200 out of 2035\n",
      "1300 out of 2035\n",
      "1400 out of 2035\n",
      "1500 out of 2035\n",
      "1600 out of 2035\n",
      "1700 out of 2035\n",
      "1800 out of 2035\n",
      "1900 out of 2035\n",
      "2000 out of 2035\n",
      "Train\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-7db992eda0ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mcreate_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'Train'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mcreate_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-3cd59f7d964f>\u001b[0m in \u001b[0;36mcreate_corpus\u001b[0;34m(split)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Extract intention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'intention'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'intention'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Update corpus with processed texts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'label'"
     ]
    }
   ],
   "source": [
    "corpus = {\n",
    "    'train':{\n",
    "        'labels': [],\n",
    "        'raw_texts': [],\n",
    "        'texts_with_med': [],\n",
    "        'texts_with_numbered_med': []\n",
    "    },\n",
    "    'test':{\n",
    "        'labels': [],\n",
    "        'raw_texts': [],\n",
    "        'texts_with_med': [],\n",
    "        'texts_with_numbered_med': []\n",
    "    },\n",
    "    'rcp': rcp,\n",
    "    'components': components\n",
    "}\n",
    "print 'Test'\n",
    "create_corpus('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "100 out of 8028\n",
      "200 out of 8028\n",
      "300 out of 8028\n",
      "400 out of 8028\n",
      "500 out of 8028\n",
      "600 out of 8028\n",
      "700 out of 8028\n",
      "800 out of 8028\n",
      "900 out of 8028\n",
      "1000 out of 8028\n",
      "1100 out of 8028\n",
      "1200 out of 8028\n",
      "1300 out of 8028\n",
      "1400 out of 8028\n",
      "1500 out of 8028\n",
      "1600 out of 8028\n",
      "1700 out of 8028\n",
      "1800 out of 8028\n",
      "1900 out of 8028\n",
      "2000 out of 8028\n",
      "2100 out of 8028\n",
      "2200 out of 8028\n",
      "2300 out of 8028\n",
      "2400 out of 8028\n",
      "2500 out of 8028\n",
      "2600 out of 8028\n",
      "2700 out of 8028\n",
      "2800 out of 8028\n",
      "2900 out of 8028\n",
      "3000 out of 8028\n",
      "3100 out of 8028\n",
      "3200 out of 8028\n",
      "3300 out of 8028\n",
      "3400 out of 8028\n",
      "3500 out of 8028\n",
      "3600 out of 8028\n",
      "3700 out of 8028\n",
      "3800 out of 8028\n",
      "4000 out of 8028\n",
      "4100 out of 8028\n",
      "4200 out of 8028\n",
      "4300 out of 8028\n",
      "4400 out of 8028\n",
      "4500 out of 8028\n",
      "4600 out of 8028\n",
      "4700 out of 8028\n",
      "4800 out of 8028\n",
      "4900 out of 8028\n",
      "5000 out of 8028\n",
      "5100 out of 8028\n",
      "5200 out of 8028\n",
      "5300 out of 8028\n",
      "5400 out of 8028\n",
      "5500 out of 8028\n",
      "5600 out of 8028\n",
      "5700 out of 8028\n",
      "5800 out of 8028\n",
      "5900 out of 8028\n",
      "6000 out of 8028\n",
      "6100 out of 8028\n",
      "6200 out of 8028\n",
      "6300 out of 8028\n",
      "6400 out of 8028\n",
      "6500 out of 8028\n",
      "6600 out of 8028\n",
      "6700 out of 8028\n",
      "6800 out of 8028\n",
      "6900 out of 8028\n",
      "7000 out of 8028\n",
      "7100 out of 8028\n",
      "7200 out of 8028\n",
      "7300 out of 8028\n",
      "7400 out of 8028\n",
      "7500 out of 8028\n",
      "7600 out of 8028\n",
      "7700 out of 8028\n",
      "7800 out of 8028\n",
      "7900 out of 8028\n",
      "8000 out of 8028\n"
     ]
    }
   ],
   "source": [
    "print 'Train'\n",
    "create_corpus('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pkl.dump(corpus, open('data/corpus.pkl', 'w'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
