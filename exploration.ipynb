{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {'train': {'question':[], 'intention':[]},\n",
    "           'test': {'question':[]}}\n",
    "\n",
    "with open('data/input_train.csv') as csvfile:\n",
    "    index = 0\n",
    "    reader = csv.reader(csvfile, delimiter=';', )\n",
    "    reader.next()\n",
    "    for row in reader:\n",
    "        assert index == int(row[0]), '{} {}'.format(index, row[0])\n",
    "        dataset['train']['question'].append(row[1])\n",
    "        index += 1\n",
    "\n",
    "with open('data/output_train.csv') as csvfile:\n",
    "    index = 0\n",
    "    reader = csv.reader(csvfile, delimiter=';', )\n",
    "    reader.next()\n",
    "    for row in reader:\n",
    "        assert index == int(row[0]), '{} {}'.format(index, row[0])\n",
    "        dataset['train']['intention'].append(row[1])\n",
    "        index += 1\n",
    "\n",
    "with open('data/input_test.csv') as csvfile:\n",
    "    index = len(dataset['train']['question'])\n",
    "    reader = csv.reader(csvfile, delimiter=';', )\n",
    "    reader.next()\n",
    "    for row in reader:\n",
    "        assert index == int(row[0]), '{} {}'.format(index, row[0])\n",
    "        dataset['test']['question'].append(row[1])\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore class balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print dataset['intention'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load RCP and components dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rcp = {}\n",
    "components = {}\n",
    "with open('data/CIS.txt') as textfile:\n",
    "    lines = map(lambda l: l.strip('\\r\\n').split('\\t'), textfile.readlines())\n",
    "    for line in lines:\n",
    "        code_cis, name = int(line[0]), line[1]\n",
    "        rcp[code_cis] = {\n",
    "            'name': name,\n",
    "            'commercialization': line[-3] == 'Commercialis\\xe9e',\n",
    "            'composition': []\n",
    "        }\n",
    "\n",
    "with open('data/COMPO.txt') as textfile:\n",
    "    lines = map(lambda l: l.strip('\\r\\n').split('\\t'), textfile.readlines())\n",
    "    for line in lines:\n",
    "        code_cis, code_compo, name, type = int(line[0]), int(line[2]), line[3], line[1]\n",
    "        rcp[code_cis]['composition'].append(code_compo)\n",
    "        components[code_compo] = {\n",
    "            'name': name,\n",
    "            'type': type\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean names to only keep drug name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "subre = '&|mg|ml|mL|POUR CENT|POUR MILLE|h|heures|g|\\%|UI|U.I.|SANS SUCRE|microgrammes|I.V.|dose|ENFANTS|IM|IR|ENFANTS ET NOURRISSONS|ADULTES|U.CEIP'\n",
    "for cis_code in rcp:\n",
    "    #print rcp[cis_code]['name']\n",
    "    rcp[cis_code]['clean_name'] = rcp[cis_code]['name'].split(' ')[0].strip(' ,')\n",
    "    #print rcp[cis_code]['clean_name']\n",
    "    '''\n",
    "    match = re.match(r\"(([A-Z.\\/ \\(\\)\\-])+)([0-9,. \\/]*(\"+ subre +\")[\\/]*)*, .*\", rcp[cis_code]['name'])\n",
    "    if match:\n",
    "        rcp[cis_code]['clean_name'] = match.group(1).split(' ')[0].strip(' ')\n",
    "        #print rcp[cis_code]['clean_name']\n",
    "    else:\n",
    "        #print rcp[cis_code]['name']\n",
    "        pass\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'composition': [83934], 'clean_name': 'ROPIVACA\\xcfNE', 'name': 'ROPIVACA\\xcfNE KABI 10 mg/ml, solution injectable en ampoule', 'commercialization': True}\n"
     ]
    }
   ],
   "source": [
    "print rcp[69133501]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract and clean sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ftfy\n",
    "import spacy\n",
    "import pickle as pkl\n",
    "from textacy import preprocess_text\n",
    "\n",
    "nlp = spacy.load('fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(split):\n",
    "    for idx in range(len(dataset[split]['question'])):\n",
    "        if idx+1 > 20:\n",
    "            pass\n",
    "        if (idx+1) % 100 == 0:\n",
    "            print '{} out of {}'.format(idx+1, len(dataset[split]['question']))\n",
    "\n",
    "        # Preprocess question\n",
    "        question = dataset[split]['question'][idx]\n",
    "        question = preprocess_text(question.decode('utf-8'), fix_unicode=True, no_accents=True, lowercase=True)\n",
    "        tokens = [token.lower_ for token in nlp(question)]\n",
    "        \n",
    "        # Extract intention\n",
    "        if dataset[split].has_key('intention'):\n",
    "            corpus[split]['label'].append(dataset[split]['intention'][idx])\n",
    "            \n",
    "        # Update corpus with processed texts\n",
    "        corpus[split]['raw_texts'].append(question)\n",
    "        \n",
    "        med_idx = 0\n",
    "        for cis_code in rcp:\n",
    "            if rcp[cis_code].has_key('clean_name') and rcp[cis_code]['commercialization']:\n",
    "                if len(rcp[cis_code]['clean_name']) <= 4:\n",
    "                    continue\n",
    "\n",
    "                _, encoding = ftfy.guess_bytes(rcp[cis_code]['clean_name'])\n",
    "                cis_name = preprocess_text(rcp[cis_code]['clean_name'].decode(encoding),\n",
    "                                           fix_unicode=True, no_accents=True, lowercase=True)\n",
    "\n",
    "                if cis_name in tokens:\n",
    "                    question = question.replace(cis_name, 'MED{}'.format(med_idx))\n",
    "                    tokens.remove(cis_name)\n",
    "                    med_idx += 1\n",
    "        \n",
    "        #print dataset[split]['question'][idx]\n",
    "        #print question\n",
    "\n",
    "        corpus[split]['texts_with_numbered_med'].append(question)\n",
    "        corpus[split]['texts_with_med'].append(re.sub(r'MED[0-9]', u'MED', question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-4f16794663b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;34m'components'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcomponents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m }\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mcreate_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mcreate_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-ea16b6954c5f>\u001b[0m in \u001b[0;36mcreate_corpus\u001b[0;34m(split)\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mftfy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mguess_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrcp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcis_code\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 cis_name = preprocess_text(rcp[cis_code]['clean_name'].decode(encoding),\n\u001b[0;32m---> 28\u001b[0;31m                                            fix_unicode=True, no_accents=True, lowercase=True)\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcis_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/romain/work/dl-env/local/lib/python2.7/site-packages/textacy/preprocess.pyc\u001b[0m in \u001b[0;36mpreprocess_text\u001b[0;34m(text, fix_unicode, lowercase, transliterate, no_urls, no_emails, no_phone_numbers, no_numbers, no_currency_symbols, no_punct, no_contractions, no_accents)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpack_contractions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mno_accents\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_accents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'unicode'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mno_punct\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_punct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/romain/work/dl-env/local/lib/python2.7/site-packages/textacy/preprocess.pyc\u001b[0m in \u001b[0;36mremove_accents\u001b[0;34m(text, method)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \"\"\"\n\u001b[1;32m    172\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'unicode'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         return ''.join(c for c in unicodedata.normalize('NFKD', text)\n\u001b[0m\u001b[1;32m    174\u001b[0m                        if not unicodedata.combining(c))\n\u001b[1;32m    175\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'ascii'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "corpus = {\n",
    "    'train':{\n",
    "        'labels': [],\n",
    "        'raw_texts': [],\n",
    "        'texts_with_med': [],\n",
    "        'texts_with_numbered_med': []\n",
    "    },\n",
    "    'test':{\n",
    "        'labels': [],\n",
    "        'raw_texts': [],\n",
    "        'texts_with_med': [],\n",
    "        'texts_with_numbered_med': []\n",
    "    },\n",
    "    'rcp': rcp,\n",
    "    'components': components\n",
    "}\n",
    "create_corpus('test')\n",
    "create_corpus('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(corpus, open('data/corpus.pkl', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pkl.load(open('data/corpus.pkl', 'r'))\n",
    "inputs = data['texts_with_numbered_med']\n",
    "labels = data['labels']\n",
    "nb_train = 7000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    tokens = nlp(text)\n",
    "    \n",
    "    return [token.lower_ for token in tokens]\n",
    "    #return [token.lower_ for token in tokens if not token.is_punct and not token.is_space and not token.is_stop]\n",
    "    #return [token.lemma_ for token in tokens if not token.is_punct and not token.is_space and not token.is_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = CountVectorizer(preprocessor=None, tokenizer=tokenizer)\n",
    "bow_feats = bow.fit_transform(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(kernel='linear').fit(bow_feats[:nb_train], labels[:nb_train])\n",
    "print svc.score(bow_feats[:nb_train], labels[:nb_train])\n",
    "print svc.score(bow_feats[nb_train:], labels[nb_train:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer()\n",
    "tfidf_feats = tfidf.fit_transform(bow_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(kernel='linear').fit(tfidf_feats[:nb_train], labels[:nb_train])\n",
    "print svc.score(tfidf_feats[:nb_train], labels[:nb_train])\n",
    "print svc.score(tfidf_feats[nb_train:], labels[nb_train:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 7600\n",
    "print bow.build_tokenizer()(inputs[idx])\n",
    "print svc.predict(tfidf_feats[idx])\n",
    "print labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print data['rcp'][66745607]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
